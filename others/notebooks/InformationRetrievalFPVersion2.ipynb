{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IbCgQTA6nIf"
   },
   "source": [
    "# I- Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqgHhBL26wMn"
   },
   "source": [
    "# II- Utils classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "81mkogAHWLVr"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the stopwords\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(text: str, lang: str = \"english\") -> List[str]:\n",
    "\n",
    "        if lang == \"all\":\n",
    "            tmp_lang = detect(text)\n",
    "            lang = \"english\" if tmp_lang == \"en\" else \"italian\"\n",
    "\n",
    "        if lang not in stopwords.fileids():\n",
    "            raise ValueError(\n",
    "                f\"Language '{lang}' is not supported. The language \\\n",
    "                should be one of the following: {stopwords.fileids()}\"\n",
    "            )\n",
    "\n",
    "        # Lowercase the text\n",
    "        text = text.lower()\n",
    "\n",
    "        # Replace ampersand with 'and'\n",
    "        text = text.replace(\"&\", \" and \")\n",
    "\n",
    "        # Normalize special characters (smart quotes, dashes, etc.)\n",
    "        text = text.translate(str.maketrans(\"‘’´“”–-\", \"'''\\\"\\\"--\"))\n",
    "\n",
    "        # Remove unnecessary periods in acronyms\n",
    "        text = re.sub(r\"\\.(?!(\\S[^. ])|\\d)\", \"\", text)\n",
    "\n",
    "        # Remove punctuation and replace with spaces\n",
    "        text = text.translate(\n",
    "            str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
    "        )\n",
    "\n",
    "        # Tokenize using NLTK (language aware)\n",
    "        tokens = word_tokenize(text, language=lang)\n",
    "\n",
    "        # Remove stopwords for the given language\n",
    "        stop_words = set(stopwords.words(lang))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Stemming\n",
    "        stemmer = SnowballStemmer(lang)\n",
    "\n",
    "        # Stem the tokens\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def profile(f):\n",
    "        def f_timer(*args, **kwargs):\n",
    "            start = time.time()\n",
    "            result = f(*args, **kwargs)\n",
    "            end = time.time()\n",
    "            elapsed_time = end - start\n",
    "\n",
    "            if elapsed_time >= 60:  # If the time is more than a minute\n",
    "                minutes = int(elapsed_time // 60)\n",
    "                seconds = elapsed_time % 60\n",
    "                print(f\"{f.__name__}: {minutes} min {seconds:.3f} s\")\n",
    "            elif elapsed_time >= 1:  # If the time is more than a second\n",
    "                print(f\"{f.__name__}: {elapsed_time:.3f} s\")\n",
    "            else:  # If the time is less than a second\n",
    "                print(f\"{f.__name__}: {elapsed_time * 1000:.3f} ms\")\n",
    "\n",
    "            return result\n",
    "\n",
    "        return f_timer\n",
    "\n",
    "\n",
    "class InvertedIndexManager:\n",
    "\n",
    "    @staticmethod\n",
    "    def load_index(input_file: str):\n",
    "\n",
    "        input_file_path = Path(input_file)\n",
    "        if not input_file_path.exists():\n",
    "            raise ValueError(\n",
    "                f\"Input file {input_file} does not exist.\\\n",
    "                     Make sure the path is correct.\"\n",
    "            )\n",
    "\n",
    "        if not input_file_path.is_file():\n",
    "            raise ValueError(\n",
    "                f\"Input file {input_file} is not a file. \\\n",
    "                    Make sure to provide a file as input.\"\n",
    "            )\n",
    "\n",
    "        # Load the index from the pickle file\n",
    "        with open(input_file_path, \"rb\") as f:\n",
    "            lexicon, inv, doc_index, stats = pickle.load(f)\n",
    "\n",
    "        return lexicon, inv, doc_index, stats\n",
    "\n",
    "    @staticmethod\n",
    "    def save_index(\n",
    "        output_folder_path: Path,\n",
    "        lexicon: dict,\n",
    "        inv_d: dict,\n",
    "        inv_f: dict,\n",
    "        doc_index: list,\n",
    "        stats: dict,\n",
    "    ):\n",
    "\n",
    "        # Save the results as pickle files\n",
    "        with open(f\"{output_folder_path}/index.pkl\", \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                (lexicon, {\"docids\": inv_d, \"freqs\": inv_f}, doc_index, stats),\n",
    "                f,\n",
    "            )\n",
    "\n",
    "        # Save each part to a separate JSONL file\n",
    "        with open(\n",
    "            f\"{output_folder_path}/lexicon.json\", \"w\", encoding=\"utf-8\"\n",
    "        ) as lex_file:\n",
    "            lex_file.write(json.dumps(lexicon))\n",
    "\n",
    "        with open(\n",
    "            f\"{output_folder_path}/inverted_file.jsonl\", \"w\", encoding=\"utf-8\"\n",
    "        ) as inv_file:\n",
    "            inv_file.write(json.dumps({\"docids\": inv_d, \"freqs\": inv_f}))\n",
    "\n",
    "        with open(\n",
    "            f\"{output_folder_path}/doc_index.jsonl\", \"w\", encoding=\"utf-8\"\n",
    "        ) as doc_file:\n",
    "            doc_file.write(json.dumps(doc_index, ensure_ascii=False))\n",
    "\n",
    "        with open(\n",
    "            f\"{output_folder_path}/stats.json\", \"w\", encoding=\"utf-8\"\n",
    "        ) as stats_file:\n",
    "            json.dump(stats, stats_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUFmQq9w7FTt",
    "outputId": "16f2642e-0cb4-4f2d-b7da-0a6da73b0585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sampl', 'text', 'test', 'preprocessor', 'function']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a sample text for testing the preprocessor function.\"\n",
    "tokens = Preprocessor.preprocess(text, lang=\"all\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9k4Nx6Z7Tmw"
   },
   "source": [
    "# III- Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AWrfl1K7Vylz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali/pythonBasicPractices/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class Indexing:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_folder: str,\n",
    "        output_folder: str,\n",
    "        lang: Literal[\"en\", \"it\", \"all\"],  # noqa\n",
    "    ) -> None:\n",
    "\n",
    "        input_folder_path = Path(input_folder)\n",
    "        if not input_folder_path.exists():\n",
    "            raise ValueError(\n",
    "                f\"Input folder {input_folder} does not exist.\\\n",
    "                     Make sure the path is correct.\"\n",
    "            )\n",
    "\n",
    "        if not input_folder_path.is_dir():\n",
    "            raise ValueError(\n",
    "                f\"Input folder {input_folder} is not a directory. \\\n",
    "                    Make sure to provide a directory as input.\"\n",
    "            )\n",
    "\n",
    "        # Get all files that end with .jsonl\n",
    "        jsonl_files = list(input_folder_path.glob(\"*.jsonl\"))\n",
    "\n",
    "        if len(jsonl_files) == 0:\n",
    "            raise ValueError(\n",
    "                f\"No .jsonl files found in the input folder {input_folder}. \\\n",
    "                    Make sure to provide a folder with .jsonl files.\"\n",
    "            )\n",
    "\n",
    "        # Filter files based on language\n",
    "        if lang != \"all\":\n",
    "            self.lang = \"english\" if lang == \"en\" else \"italian\"\n",
    "            jsonl_files = [\n",
    "                file for file in jsonl_files if f\".{lang}\" in file.suffixes[1:]\n",
    "            ]\n",
    "        else:\n",
    "            self.lang = \"all\"\n",
    "\n",
    "        self.input_folder = input_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.input_files = jsonl_files\n",
    "\n",
    "        # Initialize data structures\n",
    "        # \"term\": [docid, doc_freq, col_freq] where doc_freq is the number of\n",
    "        # documents in which the term appears and col_freq is the total number\n",
    "        # of times the term appears in the collection\n",
    "        self.lexicon = {}\n",
    "        self.doc_index = {}  # Document index\n",
    "        self.inv_d = defaultdict(list)  # TermID to list of DocIDs\n",
    "        # TermID to list of term frequencies in each DocID\n",
    "        self.inv_f = defaultdict(list)\n",
    "        self.termid = 0  # TermID counter\n",
    "\n",
    "        self.num_docs = 0  # Number of documents\n",
    "        self.total_dl = 0  # Total document length\n",
    "        self.total_toks = 0  # Total number of tokens\n",
    "\n",
    "    @Preprocessor.profile\n",
    "    def build_index(self):\n",
    "\n",
    "        # Create the output folder if it does not exist\n",
    "        output_folder_path = Path(self.output_folder)\n",
    "        if not output_folder_path.exists():\n",
    "            output_folder_path.mkdir(parents=True)\n",
    "\n",
    "        for fileid, file in tqdm(\n",
    "            enumerate(self.input_files),\n",
    "            desc=\"Indexing Files\",\n",
    "            total=len(self.input_files),\n",
    "        ):\n",
    "            # Open and read the JSONL file\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as file_content:\n",
    "                for line in file_content:\n",
    "                    doc = json.loads(line)  # Parse JSON line\n",
    "                    # Assign a new docid incrementally\n",
    "                    docid = len(self.doc_index)\n",
    "                    # Tokenize and preprocess text\n",
    "                    tokens = Preprocessor.preprocess(doc[\"text\"], self.lang)\n",
    "                    # Count term frequencies in the document\n",
    "                    token_tf = Counter(tokens)\n",
    "\n",
    "                    # Update lexicon, inverted file, and document index\n",
    "                    for token, tf in token_tf.items():\n",
    "                        # Add term to lexicon if not already present\n",
    "                        if token not in self.lexicon:\n",
    "                            # [termid, doc_freq, col_freq] i.e. termid is the term identifier, # noqa\n",
    "                            #  doc_freq is the number of documents in which the term appears, # noqa\n",
    "                            # and col_freq is the total number of times the term appears in the # noqa\n",
    "                            # collection\n",
    "                            self.lexicon[token] = [\n",
    "                                self.termid,\n",
    "                                0,\n",
    "                                0,\n",
    "                            ]\n",
    "                            # Initialize posting lists\n",
    "                            self.inv_d[self.termid], self.inv_f[self.termid] = (  # noqa\n",
    "                                [],\n",
    "                                [],\n",
    "                            )  # noqa\n",
    "                            self.termid += 1  # Increment termid\n",
    "\n",
    "                        # Update posting lists and term frequency\n",
    "                        token_id = self.lexicon[token][0]  # Get termid\n",
    "                        # Add docid to posting list\n",
    "                        self.inv_d[token_id].append(docid)\n",
    "                        # Add term frequency in posting list\n",
    "                        self.inv_f[token_id].append(tf)\n",
    "                        # Increment document frequency i.e the number of\n",
    "                        # documents in which the term appears\n",
    "                        self.lexicon[token][1] += 1\n",
    "                        # Increment collection frequency i.e the total\n",
    "                        # number of times the term appears in the collection\n",
    "                        self.lexicon[token][2] += tf\n",
    "\n",
    "                    # Update document index\n",
    "                    doclen = len(tokens)  # Document length\n",
    "                    self.doc_index[docid] = {\n",
    "                        \"doclen\": doclen,\n",
    "                        \"url\": doc[\"url\"],\n",
    "                        \"title\": doc[\"title\"],\n",
    "                    }\n",
    "                    self.total_dl += doclen\n",
    "                    self.num_docs += 1\n",
    "\n",
    "        # Properties file with collection statistics\n",
    "        stats = {\n",
    "            \"num_docs\": len(self.doc_index),\n",
    "            \"num_terms\": len(self.lexicon),\n",
    "            \"total_tokens\": self.total_dl,\n",
    "        }\n",
    "\n",
    "        InvertedIndexManager.save_index(\n",
    "            output_folder_path=output_folder_path,\n",
    "            lexicon=self.lexicon,\n",
    "            inv_d=self.inv_d,\n",
    "            inv_f=self.inv_f,\n",
    "            doc_index=self.doc_index,\n",
    "            stats=stats,\n",
    "        )\n",
    "\n",
    "        return self.lexicon, {\"docids\": self.inv_d, \"freqs\": self.inv_f}, self.doc_index, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "bf18bad49c114c1780ca325929e57d6f",
      "06bcf1eeb3d4415b8e233f1f43ed7de0",
      "e4655895e20942d08c50dfd4f775258c",
      "c1957473b2ce497da6d23d43e06f1479",
      "70e394a0ba8c4db09170ab944b002208",
      "95207e9824e145ffbabaed70fb24bbde",
      "7380c4cbacfe48658f492f1e953c8b16",
      "a0a161db8cce4b7cb709b3203b3d06ad",
      "7abd9673a02c489ba13883af145232d2",
      "121b8143dcb64227a0f745e18cb33394",
      "e5cc842dc26d40609c25474ce3647979"
     ]
    },
    "id": "DNvYgVlp7lO9",
    "outputId": "799f626c-c104-48d8-a04c-ab7b11abd038"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Files:   0%|          | 0/34 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing Files: 100%|██████████| 34/34 [09:39<00:00, 17.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_index: 9 min 41.932 s\n"
     ]
    }
   ],
   "source": [
    "# Replace argparse with direct assignments\n",
    "input_folder = \"/home/ali/pythonBasicPractices/Bravehearts1/data/documents\"  # Replace with your actual input folder path\n",
    "output_folder = \"/home/ali/pythonBasicPractices/outputO2\"  # Replace with your actual output folder path\n",
    "lang = \"all\"  # Set language as \"en\", \"it\", or \"all\"\n",
    "\n",
    "# Instantiate and run the Indexing class\n",
    "indexer = Indexing(input_folder, output_folder, lang)\n",
    "lexicon, inv, doc_index, stats = indexer.build_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdW2pkju8k3d",
    "outputId": "1591f680-f538-4b77-ab7a-b1ed74b84364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics\n",
      "{'num_docs': 27306, 'num_terms': 75764, 'total_tokens': 11134472}\n"
     ]
    }
   ],
   "source": [
    "print(\"Statistics\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ud6aQu328xZG"
   },
   "source": [
    "# IV - Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rfdk0v1ajmGK"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import math\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "\n",
    "    def __init__(self, lex, inv, doc, stats):\n",
    "        self.lexicon = lex\n",
    "        self.inv = inv\n",
    "        self.doc = doc\n",
    "        self.stats = stats\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.stats[\"num_docs\"]\n",
    "\n",
    "    def get_posting(self, termid):\n",
    "        # Find the term corresponding to the given termid\n",
    "        term = next(key for key, value in self.lexicon.items() if value[0] == termid)\n",
    "        \n",
    "        # Get the doc_freq (document frequency) from the lexicon\n",
    "        doc_freq = self.lexicon[term][1]\n",
    "        \n",
    "        # Get the total number of documents\n",
    "        total_docs = self.stats[\"num_docs\"]\n",
    "\n",
    "        # Calculate the Avg Document Length\n",
    "        avgdoclen = self.stats[\"total_tokens\"] / self.stats[\"num_docs\"]\n",
    "        \n",
    "        return InvertedIndex.PostingListIterator(\n",
    "            self.inv[\"docids\"][termid],\n",
    "            self.inv[\"freqs\"][termid],\n",
    "            self.doc,\n",
    "            doc_freq,\n",
    "            total_docs,\n",
    "            avgdoclen\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_termids(self, tokens):\n",
    "        return [\n",
    "            self.lexicon[token][0] for token in tokens if token in self.lexicon\n",
    "        ]  # noqa\n",
    "\n",
    "    def get_postings(self, termids):\n",
    "        return [self.get_posting(termid) for termid in termids]\n",
    "\n",
    "\n",
    "    class PostingListIterator:\n",
    "        def __init__(self, docids, freqs, doc, doc_freq, total_docs, avgdoclen, k1 = 1.2, b = 0.75):\n",
    "            \"\"\"\n",
    "            Initialize the PostingListIterator.\n",
    "\n",
    "            Parameters:\n",
    "                docids (list): List of document IDs where the term appears.\n",
    "                freqs (list): List of term frequencies in the corresponding documents.\n",
    "                doc (dict): Document index with metadata for each document.\n",
    "                doc_freq (int): Document frequency of the term.\n",
    "                total_docs (int): Total number of documents in the collection.\n",
    "            \"\"\"\n",
    "            self.docids = docids\n",
    "            self.freqs = freqs\n",
    "            self.pos = 0\n",
    "            self.doc = doc\n",
    "            self.doc_freq = doc_freq\n",
    "            self.total_docs = total_docs\n",
    "            self.avgdoclen = avgdoclen\n",
    "            self.k1 = k1\n",
    "            self.b = b\n",
    "\n",
    "        def docid(self):\n",
    "            if self.is_end_list():\n",
    "                return math.inf\n",
    "            return self.docids[self.pos]\n",
    "\n",
    "        def score(self):\n",
    "            # if self.is_end_list():\n",
    "            #     return 0\n",
    "            # docid = self.docids[self.pos]\n",
    "            # tf = self.freqs[self.pos]  # Term frequency in the document\n",
    "            # doclen = self.doc[docid][\"doclen\"]  # Document length\n",
    "            # if self.doc_freq == 0:  # Avoid division by zero\n",
    "            #     return 0\n",
    "            # idf = math.log(self.total_docs / self.doc_freq)\n",
    "            # tf = 1 + (math.log(tf))\n",
    "            # return tf * idf\n",
    "\n",
    "            if self.is_end_list():\n",
    "                return 0\n",
    "            docid = self.docids[self.pos]\n",
    "            tf = self.freqs[self.pos]  # Term frequency in the document\n",
    "            doclen = self.doc[docid][\"doclen\"]  # Document length\n",
    "            idf = math.log((self.total_docs - self.doc_freq + 0.5) / (self.doc_freq + 0.5) + 1)\n",
    "            \n",
    "            # BM25 score calculation\n",
    "            numerator = tf * (self.k1 + 1)\n",
    "            denominator = tf + (self.k1 * (1 - self.b + (self.b * (doclen / self.avgdoclen))))\n",
    "            return idf * (numerator / denominator)\n",
    "\n",
    "        def next(self, target=None):\n",
    "            if not target:\n",
    "                if not self.is_end_list():\n",
    "                    self.pos += 1\n",
    "            else:\n",
    "                if target > self.docids():\n",
    "                    try:\n",
    "                        self.pos = self.docids.index(target, self.pos)\n",
    "                    except ValueError:\n",
    "                        self.pos = len(self.docids)\n",
    "\n",
    "        def is_end_list(self):\n",
    "            return self.pos == len(self.docids)\n",
    "\n",
    "        def len(self):\n",
    "            return len(self.docids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopQueue:\n",
    "    def __init__(self, k=10, threshold=0.0):\n",
    "        self.queue = []\n",
    "        self.k = k\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.queue)\n",
    "\n",
    "    def would_enter(self, score):\n",
    "        return score > self.threshold\n",
    "\n",
    "    def clear(self, new_threshold=None):\n",
    "        self.queue = []\n",
    "        if new_threshold:\n",
    "            self.threshold = new_threshold\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{self.size()} items, th={self.threshold} {self.queue}\"\n",
    "\n",
    "    def insert(self, docid, score):\n",
    "        if score > self.threshold:\n",
    "            if self.size() >= self.k:\n",
    "                heapq.heapreplace(self.queue, (score, docid))\n",
    "            else:\n",
    "                heapq.heappush(self.queue, (score, docid))\n",
    "            if self.size() >= self.k:\n",
    "                self.threshold = max(self.threshold, self.queue[0][0])\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Cq9KpLy7eeoM"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class QueryProcessor:\n",
    "\n",
    "    def __init__(self, index_file):\n",
    "        lex, inv, doc, stats = InvertedIndexManager.load_index(index_file)\n",
    "        self.inv_index = InvertedIndex(lex, inv, doc, stats)\n",
    "        self.doc = doc\n",
    "\n",
    "    # Conjunctive processing\n",
    "    def boolean_and(self, postings):\n",
    "        results = []\n",
    "        # We sort the posting lists from the shortest to the longest\n",
    "        postings = sorted(postings, key=lambda p: p.len())\n",
    "        # We scan sequentially through the shortest posting list only\n",
    "        current_docid = postings[0].docid()\n",
    "        while current_docid != math.inf:\n",
    "            found = True\n",
    "            # We look for the current docid is all remaining posting lists\n",
    "            for posting in postings[1:]:\n",
    "                posting.next(current_docid)\n",
    "                if posting.docid() != current_docid:\n",
    "                    found = False\n",
    "                    break\n",
    "            # If the current docid is in all posting lists, we add it to results # noqa\n",
    "            if found:\n",
    "                results.append(current_docid)\n",
    "            # We move forward in the shortest posting list\n",
    "            postings[0].next()\n",
    "            current_docid = postings[0].docid()\n",
    "        return self.prepare_final_result(docids=results)\n",
    "\n",
    "    def query_process_and(self, query: str, lang: str = \"english\"):\n",
    "        qtokens = set(Preprocessor.preprocess(query, lang))\n",
    "        qtermids = self.inv_index.get_termids(qtokens)\n",
    "        postings = self.inv_index.get_postings(qtermids)\n",
    "        return self.boolean_and(postings)\n",
    "\n",
    "    # Disjunctive processing\n",
    "    def min_docid(self, postings):\n",
    "        min_docid = math.inf\n",
    "        for p in postings:\n",
    "            if not p.is_end_list():\n",
    "                min_docid = min(p.docid(), min_docid)\n",
    "        return min_docid\n",
    "\n",
    "    def boolean_or(self, postings):\n",
    "        results = []\n",
    "        current_docid = self.min_docid(postings)\n",
    "        while current_docid != math.inf:\n",
    "            results.append(current_docid)\n",
    "            for posting in postings:\n",
    "                if posting.docid() == current_docid:\n",
    "                    posting.next()\n",
    "            current_docid = self.min_docid(postings)\n",
    "        return self.prepare_final_result(docids=results)\n",
    "\n",
    "    def query_process_or(self, query: str, lang: str = \"english\"):\n",
    "        qtokens = set(Preprocessor.preprocess(query, lang))\n",
    "        qtermids = self.inv_index.get_termids(qtokens)\n",
    "        postings = self.inv_index.get_postings(qtermids)\n",
    "        return self.boolean_or(postings)\n",
    "\n",
    "    # TAAT Algorithm\n",
    "    def taat(self, postings, k=10):\n",
    "        A = defaultdict(float)\n",
    "        for posting in postings:\n",
    "            current_docid = posting.docid()\n",
    "            while current_docid != math.inf:\n",
    "                A[current_docid] += posting.score()\n",
    "                posting.next()\n",
    "                current_docid = posting.docid()\n",
    "        top = TopQueue(k)\n",
    "        for docid, score in A.items():\n",
    "            top.insert(docid, score)\n",
    "        result = sorted(top.queue, reverse=True)\n",
    "\n",
    "        return self.prepare_final_result(scores_docids=result)\n",
    "\n",
    "    def query_process_taat(self, query, lang=\"english\"):\n",
    "        qtokens = set(Preprocessor.preprocess(query, lang))\n",
    "        qtermids = self.inv_index.get_termids(qtokens)\n",
    "        postings = self.inv_index.get_postings(qtermids)\n",
    "        return self.taat(postings)\n",
    "\n",
    "    # DAAT Algorithm\n",
    "    def daat(self, postings, k=10):\n",
    "        top = TopQueue(k)\n",
    "        current_docid = self.min_docid(postings)\n",
    "        while current_docid != math.inf:\n",
    "            score = 0\n",
    "            next_docid = math.inf\n",
    "            for posting in postings:\n",
    "                if posting.docid() == current_docid:\n",
    "                    score += posting.score()\n",
    "                    posting.next()\n",
    "                if not posting.is_end_list():\n",
    "                    next_docid = posting.docid()\n",
    "            top.insert(current_docid, score)\n",
    "            current_docid = next_docid\n",
    "        result = sorted(top.queue, reverse=True)\n",
    "        return self.prepare_final_result(scores_docids=result)\n",
    "\n",
    "    def query_process_daat(self, query: str, lang: str = \"english\"):\n",
    "        qtokens = set(Preprocessor.preprocess(query, lang))\n",
    "        qtermids = self.inv_index.get_termids(qtokens)\n",
    "        postings = self.inv_index.get_postings(qtermids)\n",
    "        return self.daat(postings)\n",
    "\n",
    "    def prepare_final_result(self, scores_docids=None, docids=None):\n",
    "\n",
    "        final_result = []\n",
    "        if docids:\n",
    "            for docid in docids:\n",
    "                doc = self.doc[docid]\n",
    "                final_result.append(\n",
    "                    {\n",
    "                        \"docid\": docid,\n",
    "                        \"title\": doc[\"title\"],\n",
    "                        \"url\": doc[\"url\"],\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            for score, docid in scores_docids:\n",
    "                doc = self.doc[docid]\n",
    "                final_result.append(\n",
    "                    {\n",
    "                        \"docid\": docid,\n",
    "                        \"title\": doc[\"title\"],\n",
    "                        \"url\": doc[\"url\"],\n",
    "                        \"score\": score,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbcMeBqh9G9A",
    "outputId": "010f05ae-fbbf-4dd9-dfea-5add3979df8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose the processing method:\n",
      "1. DAAT (Document-at-a-Time)\n",
      "2. TAAT (Term-at-a-Time)\n",
      "Type 'exit' to quit.\n",
      "\n",
      "Processing with DAAT...\n",
      "\n",
      "Search Results:\n",
      "\n",
      " docid                                                                          title                                                                                                                   url     score\n",
      "  3319 Francesco Marcelloni - Vice Rector for international cooperation and relations             https://www.unipi.it/index.php/documenti-ateneo/item/9003-francesco-marcelloni?tmpl=component&amp;print=1 13.688869\n",
      "  3318 Francesco Marcelloni - Vice Rector for international cooperation and relations                           https://www.unipi.it/index.php/governance-and-administration/item/9003-francesco-marcelloni 13.405809\n",
      "  3341 Francesco Marcelloni - Vice Rector for international cooperation and relations                                        https://www.unipi.it/index.php/documenti-ateneo/item/9003-francesco-marcelloni 13.271453\n",
      " 23895       Presentazione dei Crosslab all'Hong Kong Productivity Council | CrossLab                       https://crosslab.dii.unipi.it/news/presentazione-dei-crosslab-allhong-kong-productivity-council  8.934083\n",
      "  3338                                           Vice Rectors and Delegates 2016-2022           https://www.unipi.it/index.php/documenti-ateneo/itemlist/category/2152-vice-rectors-and-delegates-2016-2022  7.241174\n",
      "  3384                                                                     KA103 2016                https://www.unipi.it/index.php/internazionalizzazione/item/27336-ka103-2016?tmpl=component&amp;print=1  7.129197\n",
      "  3380                                                        Mobility Consortium ILO   https://www.unipi.it/index.php/internazionalizzazione/item/27640-mobility-consortium-ilo?tmpl=component&amp;print=1  7.078118\n",
      "  3407                                                                        APPRAIS                   https://www.unipi.it/index.php/internazionalizzazione/item/20657-apprais?tmpl=component&amp;print=1  7.034914\n",
      "  3386                                                      Mobility Consortium PRT 2 https://www.unipi.it/index.php/internazionalizzazione/item/27639-mobility-consortium-prt-2?tmpl=component&amp;print=1  6.846852\n",
      "  3390                                                        Mobility Consortium PRT   https://www.unipi.it/index.php/internazionalizzazione/item/27631-mobility-consortium-prt?tmpl=component&amp;print=1  6.720427\n",
      "\n",
      "Choose the processing method:\n",
      "1. DAAT (Document-at-a-Time)\n",
      "2. TAAT (Term-at-a-Time)\n",
      "Type 'exit' to quit.\n",
      "\n",
      "Processing with TAAT...\n",
      "\n",
      "Search Results:\n",
      "\n",
      " docid                                                                                                                                                                                                         title                                                                                                                                                                                            url     score\n",
      "  3319                                                                                                                                Francesco Marcelloni - Vice Rector for international cooperation and relations                                                                                      https://www.unipi.it/index.php/documenti-ateneo/item/9003-francesco-marcelloni?tmpl=component&amp;print=1 28.471486\n",
      "  3318                                                                                                                                Francesco Marcelloni - Vice Rector for international cooperation and relations                                                                                                    https://www.unipi.it/index.php/governance-and-administration/item/9003-francesco-marcelloni 27.234115\n",
      "  3341                                                                                                                                Francesco Marcelloni - Vice Rector for international cooperation and relations                                                                                                                 https://www.unipi.it/index.php/documenti-ateneo/item/9003-francesco-marcelloni 26.685052\n",
      " 23605                                                                                                                                                                                               Upcoming events                                                                                                                                                  https://www.dst.unipi.it/upcoming-events.html 20.149569\n",
      " 23606                                                                                                                                                                                                       Gallery                                                                                                                                                          https://www.dst.unipi.it/gallery.html 16.926919\n",
      " 23900                                                                                                                          ITEMs 2019 - Workshop on Information Technology, Economics and Management | CrossLab                                                                                         https://crosslab.dii.unipi.it/news/items-2019-workshop-information-technology-economics-and-management 15.666681\n",
      "    65 Gabriela Sicilia (University of La Laguna): Is your school really better than mine? An innovative proposal to perform school efficiency evaluation in a more fair way - Dipartimento di Economia e Management https://www.ec.unipi.it/eventi/gabriela-sicilia-university-of-laguna-is-your-school-really-better-than-mine-an-innovative-proposal-to-perform-school-efficiency-evaluation-in-a-more-fair-way/ 15.426717\n",
      " 23497                                                                                                                                                                                                          News                                                                                                                                             https://www.dst.unipi.it/news-wgf-lm.html?start=48 14.688759\n",
      " 23645                                                                                                                                              Practical insights and techniques in seismic velocity estimation                                                            https://www.dst.unipi.it/item/1113-practical-insights-and-techniques-in-seismic-velocity-estimation.html?tmpl=component&amp;print=1 14.328453\n",
      "  3502                                                                                                                                                                                            R 4 aRchaeologists                                                                                                https://www.unipi.it/index.php/humanities/item/16574-r4rchaeologists?tmpl=component&amp;print=1 14.166898\n",
      "\n",
      "Choose the processing method:\n",
      "1. DAAT (Document-at-a-Time)\n",
      "2. TAAT (Term-at-a-Time)\n",
      "Type 'exit' to quit.\n",
      "Exiting the search engine. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_folder = \"/home/ali/pythonBasicPractices/outputO2/index.pkl\"\n",
    "query_processor = QueryProcessor(input_folder)\n",
    "\n",
    "while True:\n",
    "  # Prompt the user to select the processing method\n",
    "  print(\"\\nChoose the processing method:\")\n",
    "  print(\"1. DAAT (Document-at-a-Time)\")\n",
    "  print(\"2. TAAT (Term-at-a-Time)\")\n",
    "  print(\"Type 'exit' to quit.\")\n",
    "\n",
    "  choice = input(\"Enter your choice (1 or 2): \").strip()\n",
    "\n",
    "  # Exit condition\n",
    "  if choice.lower() == \"exit\":\n",
    "      print(\"Exiting the search engine. Goodbye!\")\n",
    "      break\n",
    "\n",
    "  # Validate choice\n",
    "  if choice not in [\"1\", \"2\"]:\n",
    "      print(\"Invalid choice. Please enter 1 for DAAT or 2 for TAAT.\")\n",
    "      continue\n",
    "\n",
    "  # Prompt the user to input a query\n",
    "  query = input(\"\\nEnter your query: \").strip()\n",
    "  \n",
    "\n",
    "  try:\n",
    "      # Process the query based on user choice\n",
    "      if choice == \"1\":\n",
    "          print(\"\\nProcessing with DAAT...\")\n",
    "          results = query_processor.query_process_daat(query)\n",
    "      elif choice == \"2\":\n",
    "          print(\"\\nProcessing with TAAT...\")\n",
    "          results = query_processor.query_process_taat(query)\n",
    "\n",
    "      # Display results\n",
    "      if not results:\n",
    "          print(\"No results found for the query.\")\n",
    "      else:\n",
    "          # Display results in a table format using pandas\n",
    "          df = pd.DataFrame(results)\n",
    "          print(\"\\nSearch Results:\\n\")\n",
    "          print(df.to_string(index=False))\n",
    "  except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRIBhj1zAQm7"
   },
   "source": [
    "# V- Evaluation"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06bcf1eeb3d4415b8e233f1f43ed7de0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95207e9824e145ffbabaed70fb24bbde",
      "placeholder": "​",
      "style": "IPY_MODEL_7380c4cbacfe48658f492f1e953c8b16",
      "value": "Indexing Files: 100%"
     }
    },
    "121b8143dcb64227a0f745e18cb33394": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70e394a0ba8c4db09170ab944b002208": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7380c4cbacfe48658f492f1e953c8b16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7abd9673a02c489ba13883af145232d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "95207e9824e145ffbabaed70fb24bbde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0a161db8cce4b7cb709b3203b3d06ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf18bad49c114c1780ca325929e57d6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_06bcf1eeb3d4415b8e233f1f43ed7de0",
       "IPY_MODEL_e4655895e20942d08c50dfd4f775258c",
       "IPY_MODEL_c1957473b2ce497da6d23d43e06f1479"
      ],
      "layout": "IPY_MODEL_70e394a0ba8c4db09170ab944b002208"
     }
    },
    "c1957473b2ce497da6d23d43e06f1479": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_121b8143dcb64227a0f745e18cb33394",
      "placeholder": "​",
      "style": "IPY_MODEL_e5cc842dc26d40609c25474ce3647979",
      "value": " 34/34 [16:01&lt;00:00, 26.91s/it]"
     }
    },
    "e4655895e20942d08c50dfd4f775258c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0a161db8cce4b7cb709b3203b3d06ad",
      "max": 34,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7abd9673a02c489ba13883af145232d2",
      "value": 34
     }
    },
    "e5cc842dc26d40609c25474ce3647979": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
