{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUopqO50q3M4",
        "outputId": "1ef266b1-37b6-4c39-9bb9-735315da2b0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m645.1/981.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=4e1a9f5c2e8245051a366bb0cef4cec8427e3e5009fb74059b2e89372c19f3b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IbCgQTA6nIf"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the collection\n",
        "!wget -O documents.tar.xz https://github.com/tderick/Bravehearts1/raw/refs/heads/main/data/documents.tar.xz\n",
        "\n",
        "# Unzip the collection\n",
        "!tar -xJvf documents.tar.xz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vonhqojvk_-N",
        "outputId": "dfe999ab-a232-4810-c309-11d4f2227dda"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-16 11:26:25--  https://github.com/tderick/Bravehearts1/raw/refs/heads/main/data/documents.tar.xz\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/tderick/Bravehearts1/refs/heads/main/data/documents.tar.xz [following]\n",
            "--2024-12-16 11:26:26--  https://raw.githubusercontent.com/tderick/Bravehearts1/refs/heads/main/data/documents.tar.xz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6776516 (6.5M) [application/octet-stream]\n",
            "Saving to: ‘documents.tar.xz’\n",
            "\n",
            "documents.tar.xz    100%[===================>]   6.46M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-12-16 11:26:26 (111 MB/s) - ‘documents.tar.xz’ saved [6776516/6776516]\n",
            "\n",
            "documents/\n",
            "documents/atricolandosi.unipi.en.jsonl\n",
            "documents/atricolandosi.unipi.it.jsonl\n",
            "documents/crosslab.dii.unipi.en.jsonl\n",
            "documents/crosslab.dii.unipi.it.jsonl\n",
            "documents/dcci.unipi.en.jsonl\n",
            "documents/dcci.unipi.it.jsonl\n",
            "documents/destec.unipi.en.jsonl\n",
            "documents/destec.unipi.it.jsonl\n",
            "documents/di.unipi.en.jsonl\n",
            "documents/di.unipi.it.jsonl\n",
            "documents/dici.unipi.en.jsonl\n",
            "documents/dici.unipi.it.jsonl\n",
            "documents/dm.unipi.en.jsonl\n",
            "documents/dm.unipi.it.jsonl\n",
            "documents/dst.unipi.en.jsonl\n",
            "documents/dst.unipi.it.jsonl\n",
            "documents/ec.unipi.en.jsonl\n",
            "documents/ec.unipi.it.jsonl\n",
            "documents/edpat.unipi.en.jsonl\n",
            "documents/edpat.unipi.it.jsonl\n",
            "documents/farm.unipi.en.jsonl\n",
            "documents/farm.unipi.it.jsonl\n",
            "documents/fileli.unipi.en.jsonl\n",
            "documents/fileli.unipi.it.jsonl\n",
            "documents/ict.unipi.en.jsonl\n",
            "documents/ict.unipi.it.jsonl\n",
            "documents/jus.unipi.en.jsonl\n",
            "documents/jus.unipi.it.jsonl\n",
            "documents/nutrafood.unipi.en.jsonl\n",
            "documents/nutrafood.unipi.it.jsonl\n",
            "documents/sp.unipi.en.jsonl\n",
            "documents/sp.unipi.it.jsonl\n",
            "documents/unipi.en.jsonl\n",
            "documents/unipi.it.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqgHhBL26wMn"
      },
      "source": [
        "# Utils classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "81mkogAHWLVr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pickle\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import nltk\n",
        "from langdetect import detect\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the stopwords\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"punkt_tab\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "\n",
        "class Preprocessor:\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(text: str, lang: str = \"english\") -> List[str]:\n",
        "\n",
        "        if lang == \"all\":\n",
        "            tmp_lang = detect(text)\n",
        "            lang = \"english\" if tmp_lang == \"en\" else \"italian\"\n",
        "\n",
        "        if lang not in stopwords.fileids():\n",
        "            raise ValueError(\n",
        "                f\"Language '{lang}' is not supported. The language \\\n",
        "                should be one of the following: {stopwords.fileids()}\"\n",
        "            )\n",
        "\n",
        "        # Lowercase the text\n",
        "        text = text.lower()\n",
        "\n",
        "        # Replace ampersand with 'and'\n",
        "        text = text.replace(\"&\", \" and \")\n",
        "\n",
        "        # Normalize special characters (smart quotes, dashes, etc.)\n",
        "        text = text.translate(str.maketrans(\"‘’´“”–-\", \"'''\\\"\\\"--\"))\n",
        "\n",
        "        # Remove unnecessary periods in acronyms\n",
        "        text = re.sub(r\"\\.(?!(\\S[^. ])|\\d)\", \"\", text)\n",
        "\n",
        "        # Remove punctuation and replace with spaces\n",
        "        text = text.translate(\n",
        "            str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
        "        )\n",
        "\n",
        "        # Tokenize using NLTK (language aware)\n",
        "        tokens = word_tokenize(text, language=lang)\n",
        "\n",
        "        # Remove stopwords for the given language\n",
        "        stop_words = set(stopwords.words(lang))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        # Stemming\n",
        "        stemmer = SnowballStemmer(lang)\n",
        "\n",
        "        # Stem the tokens\n",
        "        tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    @staticmethod\n",
        "    def profile(f):\n",
        "        def f_timer(*args, **kwargs):\n",
        "            start = time.time()\n",
        "            result = f(*args, **kwargs)\n",
        "            end = time.time()\n",
        "            elapsed_time = end - start\n",
        "\n",
        "            if elapsed_time >= 60:  # If the time is more than a minute\n",
        "                minutes = int(elapsed_time // 60)\n",
        "                seconds = elapsed_time % 60\n",
        "                print(f\"{f.__name__}: {minutes} min {seconds:.3f} s\")\n",
        "            elif elapsed_time >= 1:  # If the time is more than a second\n",
        "                print(f\"{f.__name__}: {elapsed_time:.3f} s\")\n",
        "            else:  # If the time is less than a second\n",
        "                print(f\"{f.__name__}: {elapsed_time * 1000:.3f} ms\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        return f_timer\n",
        "\n",
        "\n",
        "class InvertedIndexManager:\n",
        "\n",
        "    @staticmethod\n",
        "    def load_index(input_file: str):\n",
        "\n",
        "        input_file_path = Path(input_file)\n",
        "        if not input_file_path.exists():\n",
        "            raise ValueError(\n",
        "                f\"Input file {input_file} does not exist.\\\n",
        "                     Make sure the path is correct.\"\n",
        "            )\n",
        "\n",
        "        if not input_file_path.is_file():\n",
        "            raise ValueError(\n",
        "                f\"Input file {input_file} is not a file. \\\n",
        "                    Make sure to provide a file as input.\"\n",
        "            )\n",
        "\n",
        "        # Load the index from the pickle file\n",
        "        with open(input_file_path, \"rb\") as f:\n",
        "            lexicon, inv, doc_index, stats = pickle.load(f)\n",
        "\n",
        "        return lexicon, inv, doc_index, stats\n",
        "\n",
        "    @staticmethod\n",
        "    def save_index(\n",
        "        output_folder_path: Path,\n",
        "        lexicon: dict,\n",
        "        inv_d: dict,\n",
        "        inv_f: dict,\n",
        "        doc_index: list,\n",
        "        stats: dict,\n",
        "    ):\n",
        "\n",
        "        # Save the results as pickle files\n",
        "        with open(f\"{output_folder_path}/index.pkl\", \"wb\") as f:\n",
        "            pickle.dump(\n",
        "                (lexicon, {\"docids\": inv_d, \"freqs\": inv_f}, doc_index, stats),\n",
        "                f,\n",
        "            )\n",
        "\n",
        "        # Save each part to a separate JSONL file\n",
        "        with open(\n",
        "            f\"{output_folder_path}/lexicon.json\", \"w\", encoding=\"utf-8\"\n",
        "        ) as lex_file:\n",
        "            lex_file.write(json.dumps(lexicon))\n",
        "\n",
        "        with open(\n",
        "            f\"{output_folder_path}/inverted_file.jsonl\", \"w\", encoding=\"utf-8\"\n",
        "        ) as inv_file:\n",
        "            inv_file.write(json.dumps({\"docids\": inv_d, \"freqs\": inv_f}))\n",
        "\n",
        "        with open(\n",
        "            f\"{output_folder_path}/doc_index.jsonl\", \"w\", encoding=\"utf-8\"\n",
        "        ) as doc_file:\n",
        "            doc_file.write(json.dumps(doc_index, ensure_ascii=False))\n",
        "\n",
        "        with open(\n",
        "            f\"{output_folder_path}/stats.json\", \"w\", encoding=\"utf-8\"\n",
        "        ) as stats_file:\n",
        "            json.dump(stats, stats_file, ensure_ascii=False, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUFmQq9w7FTt",
        "outputId": "2864ba22-3987-4a90-f471-32d9ecb2ac31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sampl', 'text', 'test', 'preprocessor', 'function']\n"
          ]
        }
      ],
      "source": [
        "text = \"This is a sample text for testing the preprocessor function.\"\n",
        "tokens = Preprocessor.preprocess(text, lang=\"all\")\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9k4Nx6Z7Tmw"
      },
      "source": [
        "# Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AWrfl1K7Vylz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "from typing import Literal\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "class Indexing:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_folder: str,\n",
        "        output_folder: str,\n",
        "        lang: Literal[\"en\", \"it\", \"all\"],  # noqa\n",
        "    ) -> None:\n",
        "\n",
        "        input_folder_path = Path(input_folder)\n",
        "        if not input_folder_path.exists():\n",
        "            raise ValueError(\n",
        "                f\"Input folder {input_folder} does not exist.\\\n",
        "                     Make sure the path is correct.\"\n",
        "            )\n",
        "\n",
        "        if not input_folder_path.is_dir():\n",
        "            raise ValueError(\n",
        "                f\"Input folder {input_folder} is not a directory. \\\n",
        "                    Make sure to provide a directory as input.\"\n",
        "            )\n",
        "\n",
        "        # Get all files that end with .jsonl\n",
        "        jsonl_files = list(input_folder_path.glob(\"*.jsonl\"))\n",
        "\n",
        "        if len(jsonl_files) == 0:\n",
        "            raise ValueError(\n",
        "                f\"No .jsonl files found in the input folder {input_folder}. \\\n",
        "                    Make sure to provide a folder with .jsonl files.\"\n",
        "            )\n",
        "\n",
        "        # Filter files based on language\n",
        "        if lang != \"all\":\n",
        "            self.lang = \"english\" if lang == \"en\" else \"italian\"\n",
        "            jsonl_files = [\n",
        "                file for file in jsonl_files if f\".{lang}\" in file.suffixes[1:]\n",
        "            ]\n",
        "        else:\n",
        "            self.lang = \"all\"\n",
        "\n",
        "        self.input_folder = input_folder\n",
        "        self.output_folder = output_folder\n",
        "        self.input_files = jsonl_files\n",
        "\n",
        "        # Initialize data structures\n",
        "        # \"term\": [docid, doc_freq, col_freq] where doc_freq is the number of\n",
        "        # documents in which the term appears and col_freq is the total number\n",
        "        # of times the term appears in the collection\n",
        "        self.lexicon = {}\n",
        "        self.doc_index = {}  # Document index\n",
        "        self.inv_d = defaultdict(list)  # TermID to list of DocIDs\n",
        "        # TermID to list of term frequencies in each DocID\n",
        "        self.inv_f = defaultdict(list)\n",
        "        self.termid = 0  # TermID counter\n",
        "\n",
        "        self.num_docs = 0  # Number of documents\n",
        "        self.total_dl = 0  # Total document length\n",
        "        self.total_toks = 0  # Total number of tokens\n",
        "\n",
        "    @Preprocessor.profile\n",
        "    def build_index(self):\n",
        "\n",
        "        # Create the output folder if it does not exist\n",
        "        output_folder_path = Path(self.output_folder)\n",
        "        if not output_folder_path.exists():\n",
        "            output_folder_path.mkdir(parents=True)\n",
        "\n",
        "        for fileid, file in tqdm(\n",
        "            enumerate(self.input_files),\n",
        "            desc=\"Indexing Files\",\n",
        "            total=len(self.input_files),\n",
        "        ):\n",
        "            # Open and read the JSONL file\n",
        "            with open(file, \"r\", encoding=\"utf-8\") as file_content:\n",
        "                for line in file_content:\n",
        "                    doc = json.loads(line)  # Parse JSON line\n",
        "                    # Assign a new docid incrementally\n",
        "                    docid = len(self.doc_index)\n",
        "                    # Tokenize and preprocess text\n",
        "                    tokens = Preprocessor.preprocess(doc[\"text\"], self.lang)\n",
        "                    # Count term frequencies in the document\n",
        "                    token_tf = Counter(tokens)\n",
        "\n",
        "                    # Update lexicon, inverted file, and document index\n",
        "                    for token, tf in token_tf.items():\n",
        "                        # Add term to lexicon if not already present\n",
        "                        if token not in self.lexicon:\n",
        "                            # [termid, doc_freq, col_freq] i.e. termid is the term identifier, # noqa\n",
        "                            #  doc_freq is the number of documents in which the term appears, # noqa\n",
        "                            # and col_freq is the total number of times the term appears in the # noqa\n",
        "                            # collection\n",
        "                            self.lexicon[token] = [\n",
        "                                self.termid,\n",
        "                                0,\n",
        "                                0,\n",
        "                            ]\n",
        "                            # Initialize posting lists\n",
        "                            self.inv_d[self.termid], self.inv_f[self.termid] = (  # noqa\n",
        "                                [],\n",
        "                                [],\n",
        "                            )  # noqa\n",
        "                            self.termid += 1  # Increment termid\n",
        "\n",
        "                        # Update posting lists and term frequency\n",
        "                        token_id = self.lexicon[token][0]  # Get termid\n",
        "                        # Add docid to posting list\n",
        "                        self.inv_d[token_id].append(docid)\n",
        "                        # Add term frequency in posting list\n",
        "                        self.inv_f[token_id].append(tf)\n",
        "                        # Increment document frequency i.e the number of\n",
        "                        # documents in which the term appears\n",
        "                        self.lexicon[token][1] += 1\n",
        "                        # Increment collection frequency i.e the total\n",
        "                        # number of times the term appears in the collection\n",
        "                        self.lexicon[token][2] += tf\n",
        "\n",
        "                    # Update document index\n",
        "                    doclen = len(tokens)  # Document length\n",
        "                    self.doc_index[docid] = {\n",
        "                        \"doclen\": doclen,\n",
        "                        \"url\": doc[\"url\"],\n",
        "                        \"title\": doc[\"title\"],\n",
        "                    }\n",
        "                    self.total_dl += doclen\n",
        "                    self.num_docs += 1\n",
        "\n",
        "        # Properties file with collection statistics\n",
        "        stats = {\n",
        "            \"num_docs\": len(self.doc_index),\n",
        "            \"num_terms\": len(self.lexicon),\n",
        "            \"total_tokens\": self.total_dl,\n",
        "        }\n",
        "\n",
        "        InvertedIndexManager.save_index(\n",
        "            output_folder_path=output_folder_path,\n",
        "            lexicon=self.lexicon,\n",
        "            inv_d=self.inv_d,\n",
        "            inv_f=self.inv_f,\n",
        "            doc_index=self.doc_index,\n",
        "            stats=stats,\n",
        "        )\n",
        "\n",
        "        return self.lexicon, {\"docids\": self.inv_d, \"freqs\": self.inv_f}, self.doc_index, stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "61ecf45e9535463f83a6e0556e1b4974",
            "ee76b053100743388577ac030f706de1",
            "4128af8a1e5d44beb92ce241fc29b1f7",
            "953974af1ea141b2b82248edc847b172",
            "c914bbcefc3d43eeaf5562a05d980f4e",
            "e5fc4f741fe04732ade872ca5f0df1f9",
            "159573d1f0f54125a31c75fc2856a4a4",
            "559dd8be89c7405ca1bd261cf38afbf3",
            "ffeafdd2f8524b4195561c2a696dd35b",
            "bc0cfd13d46e4c48ba519839feb41098",
            "401f1024141d47d3b161b95e240f78af"
          ]
        },
        "id": "DNvYgVlp7lO9",
        "outputId": "3475e56d-a69e-466c-c85e-76282d0cb295"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Indexing Files:   0%|          | 0/34 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61ecf45e9535463f83a6e0556e1b4974"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Replace argparse with direct assignments\n",
        "\n",
        "input_folder = \"/content/documents\"  # Replace with your actual input folder path\n",
        "output_folder = \"/content/index\"  # Replace with your actual output folder path\n",
        "lang = \"all\"  # Set language as \"en\", \"it\", or \"all\"\n",
        "\n",
        "# Instantiate and run the Indexing class\n",
        "indexer = Indexing(input_folder, output_folder, lang)\n",
        "lexicon, inv, doc_index, stats = indexer.build_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdW2pkju8k3d"
      },
      "outputs": [],
      "source": [
        "print(\"Statistics\")\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud6aQu328xZG"
      },
      "source": [
        "# Querying"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Querying Interface"
      ],
      "metadata": {
        "id": "Lh0BpHo46cjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ipywidgets import widgets, Output, VBox, HBox, Layout\n",
        "from IPython.display import display, clear_output, HTML\n",
        "\n",
        "def create_hyperlinked_dataframe(df):\n",
        "    \"\"\"\n",
        "    Convert 'url' column in the DataFrame into clickable hyperlinks.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame containing a 'url' column.\n",
        "\n",
        "    Returns:\n",
        "        HTML: HTML representation of the DataFrame with clickable links.\n",
        "    \"\"\"\n",
        "    # Check if 'url' column exists\n",
        "    if 'url' in df.columns:\n",
        "        df['url'] = df['url'].apply(\n",
        "            lambda x: f'<a href=\"{x}\" target=\"_blank\" style=\"color: #1E90FF; text-decoration: none;\">{x}</a>'\n",
        "        )\n",
        "    return HTML(df.to_html(escape=False, index=False))\n",
        "\n",
        "def process_query(query, method, query_processor, output_area):\n",
        "    \"\"\"\n",
        "    Function to process the query based on the selected method.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The user query.\n",
        "        method (str): The processing method (\"DAAT\" or \"TAAT\").\n",
        "        output_area: The output widget for displaying results.\n",
        "    \"\"\"\n",
        "    with output_area:\n",
        "        clear_output()  # Clear previous output\n",
        "        print(f\"\\nProcessing with {method}...\")\n",
        "\n",
        "        try:\n",
        "            # Process the query based on the selected method\n",
        "            if method == \"DAAT\":\n",
        "                results = query_processor.query_process_daat(query)\n",
        "            elif method == \"TAAT\":\n",
        "                results = query_processor.query_process_taat(query)\n",
        "\n",
        "            # Display results\n",
        "            if not results:\n",
        "                print(\"\\nNo results found for the query.\")\n",
        "            else:\n",
        "                df = pd.DataFrame(results)\n",
        "                # Create hyperlinked DataFrame\n",
        "                display(HTML(\"<h4>Search Results:</h4>\"))\n",
        "                display(create_hyperlinked_dataframe(df))\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "def search_engine_ui(query_processor):\n",
        "    \"\"\"\n",
        "    Interactive UI for the search engine with Enter key support.\n",
        "    \"\"\"\n",
        "    # Dropdown to select the processing method\n",
        "    method_dropdown = widgets.Dropdown(\n",
        "        options=[\"DAAT\", \"TAAT\"],\n",
        "        value=\"DAAT\",\n",
        "        description=\"Method:\",\n",
        "        layout=Layout(width='30%')\n",
        "    )\n",
        "\n",
        "    # Text box for the query input\n",
        "    query_input = widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Enter your query',\n",
        "        description='Query:',\n",
        "        layout=Layout(width='70%')\n",
        "    )\n",
        "\n",
        "    # Button to process the query\n",
        "    search_button = widgets.Button(\n",
        "        description=\"Search\",\n",
        "        button_style='success',  # 'success', 'info', 'warning', 'danger' or ''\n",
        "        tooltip='Click to search',\n",
        "        icon='search',\n",
        "        layout=Layout(width='20%')\n",
        "    )\n",
        "\n",
        "    # Output area to display results\n",
        "    output_area = Output(layout=Layout(border='1px solid lightgray', width='100%', height='auto'))\n",
        "\n",
        "    # Event handler for the search button\n",
        "    def on_search_clicked(b=None):\n",
        "        query = query_input.value.strip()\n",
        "        method = method_dropdown.value\n",
        "\n",
        "        if not query:\n",
        "            with output_area:\n",
        "                clear_output()\n",
        "                print(\"Please enter a query.\")\n",
        "            return\n",
        "\n",
        "        # Process the query\n",
        "        process_query(query, method, query_processor, output_area)\n",
        "\n",
        "    # Link the button click event to the handler\n",
        "    search_button.on_click(on_search_clicked)\n",
        "\n",
        "    # Add \"Enter key\" support for query input\n",
        "    query_input.on_submit(on_search_clicked)\n",
        "\n",
        "    # Layout for input elements\n",
        "    input_area = HBox([query_input, method_dropdown, search_button])\n",
        "\n",
        "    # Full layout\n",
        "    ui = VBox([input_area, output_area], layout=Layout(width='100%'))\n",
        "\n",
        "    # Display the UI\n",
        "    display(ui)\n"
      ],
      "metadata": {
        "id": "cuIjXXYLJlYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generic Inverted Index and Generic Query Processor"
      ],
      "metadata": {
        "id": "4IEuxdU9u6oY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generic Inverted Index"
      ],
      "metadata": {
        "id": "Z1BkuZCl2fY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "import math\n",
        "\n",
        "class GenericInvertedIndex:\n",
        "\n",
        "    def __init__(self, lex, inv, doc, stats):\n",
        "        self.lexicon = lex\n",
        "        self.inv = inv\n",
        "        self.doc = doc\n",
        "        self.stats = stats\n",
        "\n",
        "    def num_docs(self):\n",
        "        return self.stats[\"num_docs\"]\n",
        "\n",
        "    def get_posting(self, termid):\n",
        "       return GenericInvertedIndex.PostingListIterator(\n",
        "            self.inv[\"docids\"][termid], self.inv[\"freqs\"][termid], self.doc\n",
        "        )\n",
        "\n",
        "\n",
        "    def get_termids(self, tokens):\n",
        "        return [\n",
        "            self.lexicon[token][0] for token in tokens if token in self.lexicon\n",
        "        ]  # noqa\n",
        "\n",
        "    def get_postings(self, termids):\n",
        "        return [self.get_posting(termid) for termid in termids]\n",
        "\n",
        "\n",
        "    class PostingListIterator:\n",
        "        def __init__(self, docids, freqs, doc):\n",
        "            \"\"\"\n",
        "            Initialize the PostingListIterator.\n",
        "\n",
        "            Parameters:\n",
        "                docids (list): List of document IDs where the term appears.\n",
        "                freqs (list): List of term frequencies in the corresponding documents.\n",
        "                doc (dict): Document index with metadata for each document.\n",
        "                doc_freq (int): Document frequency of the term.\n",
        "                total_docs (int): Total number of documents in the collection.\n",
        "            \"\"\"\n",
        "            self.docids = docids\n",
        "            self.freqs = freqs\n",
        "            self.pos = 0\n",
        "            self.doc = doc\n",
        "\n",
        "        def docid(self):\n",
        "            if self.is_end_list():\n",
        "                return math.inf\n",
        "            return self.docids[self.pos]\n",
        "\n",
        "        def score(self):\n",
        "            if self.is_end_list():\n",
        "                return math.inf\n",
        "            return self.freqs[self.pos] / self.doc[self.docid()][\"doclen\"]\n",
        "\n",
        "        def next(self, target=None):\n",
        "            if not target:\n",
        "                if not self.is_end_list():\n",
        "                    self.pos += 1\n",
        "            else:\n",
        "                if target > self.docids():\n",
        "                    try:\n",
        "                        self.pos = self.docids.index(target, self.pos)\n",
        "                    except ValueError:\n",
        "                        self.pos = len(self.docids)\n",
        "\n",
        "        def is_end_list(self):\n",
        "            return self.pos == len(self.docids)\n",
        "\n",
        "        def len(self):\n",
        "            return len(self.docids)"
      ],
      "metadata": {
        "id": "433QjAGdvCfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generic Querying Processor"
      ],
      "metadata": {
        "id": "kuVRYvFW1ujA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "class TopQueue:\n",
        "    def __init__(self, k=10, threshold=0.0):\n",
        "        self.queue = []\n",
        "        self.k = k\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.queue)\n",
        "\n",
        "    def would_enter(self, score):\n",
        "        return score > self.threshold\n",
        "\n",
        "    def clear(self, new_threshold=None):\n",
        "        self.queue = []\n",
        "        if new_threshold:\n",
        "            self.threshold = new_threshold\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<{self.size()} items, th={self.threshold} {self.queue}\"\n",
        "\n",
        "    def insert(self, docid, score):\n",
        "        if score > self.threshold:\n",
        "            if self.size() >= self.k:\n",
        "                heapq.heapreplace(self.queue, (score, docid))\n",
        "            else:\n",
        "                heapq.heappush(self.queue, (score, docid))\n",
        "            if self.size() >= self.k:\n",
        "                self.threshold = max(self.threshold, self.queue[0][0])\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "class GenericQueryProcessor:\n",
        "\n",
        "    def __init__(self, index_file):\n",
        "        self.lex, self.inv, self.doc, self.stats = InvertedIndexManager.load_index(index_file)\n",
        "        self.inv_index = GenericInvertedIndex(self.lex, self.inv, self.doc, self.stats)\n",
        "\n",
        "    # Conjunctive processing\n",
        "    def boolean_and(self, postings):\n",
        "        results = []\n",
        "        # We sort the posting lists from the shortest to the longest\n",
        "        postings = sorted(postings, key=lambda p: p.len())\n",
        "        # We scan sequentially through the shortest posting list only\n",
        "        current_docid = postings[0].docid()\n",
        "        while current_docid != math.inf:\n",
        "            found = True\n",
        "            # We look for the current docid is all remaining posting lists\n",
        "            for posting in postings[1:]:\n",
        "                posting.next(current_docid)\n",
        "                if posting.docid() != current_docid:\n",
        "                    found = False\n",
        "                    break\n",
        "            # If the current docid is in all posting lists, we add it to results # noqa\n",
        "            if found:\n",
        "                results.append(current_docid)\n",
        "            # We move forward in the shortest posting list\n",
        "            postings[0].next()\n",
        "            current_docid = postings[0].docid()\n",
        "        return self.prepare_final_result(docids=results)\n",
        "\n",
        "    def query_process_and(self, query: str, lang: str = \"english\"):\n",
        "        qtokens = set(Preprocessor.preprocess(query, lang))\n",
        "        qtermids = self.inv_index.get_termids(qtokens)\n",
        "        postings = self.inv_index.get_postings(qtermids)\n",
        "        return self.boolean_and(postings)\n",
        "\n",
        "    # Disjunctive processing\n",
        "    def min_docid(self, postings):\n",
        "        min_docid = math.inf\n",
        "        for p in postings:\n",
        "            if not p.is_end_list():\n",
        "                min_docid = min(p.docid(), min_docid)\n",
        "        return min_docid\n",
        "\n",
        "    def boolean_or(self, postings):\n",
        "        results = []\n",
        "        current_docid = self.min_docid(postings)\n",
        "        while current_docid != math.inf:\n",
        "            results.append(current_docid)\n",
        "            for posting in postings:\n",
        "                if posting.docid() == current_docid:\n",
        "                    posting.next()\n",
        "            current_docid = self.min_docid(postings)\n",
        "        return self.prepare_final_result(docids=results)\n",
        "\n",
        "    def query_process_or(self, query: str, lang: str = \"english\"):\n",
        "        qtokens = set(Preprocessor.preprocess(query, lang))\n",
        "        qtermids = self.inv_index.get_termids(qtokens)\n",
        "        postings = self.inv_index.get_postings(qtermids)\n",
        "        return self.boolean_or(postings)\n",
        "\n",
        "    # TAAT Algorithm\n",
        "    def taat(self, postings, k=10):\n",
        "        A = defaultdict(float)\n",
        "        for posting in postings:\n",
        "            current_docid = posting.docid()\n",
        "            while current_docid != math.inf:\n",
        "                A[current_docid] += posting.score()\n",
        "                posting.next()\n",
        "                current_docid = posting.docid()\n",
        "        top = TopQueue(k)\n",
        "        for docid, score in A.items():\n",
        "            top.insert(docid, score)\n",
        "        result = sorted(top.queue, reverse=True)\n",
        "\n",
        "        return self.prepare_final_result(scores_docids=result)\n",
        "\n",
        "    def query_process_taat(self, query, lang=\"english\"):\n",
        "        qtokens = set(Preprocessor.preprocess(query, lang))\n",
        "        qtermids = self.inv_index.get_termids(qtokens)\n",
        "        postings = self.inv_index.get_postings(qtermids)\n",
        "        return self.taat(postings)\n",
        "\n",
        "    # DAAT Algorithm\n",
        "    def daat(self, postings, k=10):\n",
        "        top = TopQueue(k)\n",
        "        current_docid = self.min_docid(postings)\n",
        "        while current_docid != math.inf:\n",
        "            score = 0\n",
        "            next_docid = math.inf\n",
        "            for posting in postings:\n",
        "                if posting.docid() == current_docid:\n",
        "                    score += posting.score()\n",
        "                    posting.next()\n",
        "                if not posting.is_end_list():\n",
        "                    next_docid = posting.docid()\n",
        "            top.insert(current_docid, score)\n",
        "            current_docid = next_docid\n",
        "        result = sorted(top.queue, reverse=True)\n",
        "        return self.prepare_final_result(scores_docids=result)\n",
        "\n",
        "    def query_process_daat(self, query: str, lang: str = \"english\"):\n",
        "        qtokens = set(Preprocessor.preprocess(query, lang))\n",
        "        qtermids = self.inv_index.get_termids(qtokens)\n",
        "        postings = self.inv_index.get_postings(qtermids)\n",
        "        return self.daat(postings)\n",
        "\n",
        "    def prepare_final_result(self, scores_docids=None, docids=None):\n",
        "\n",
        "        final_result = []\n",
        "        if docids:\n",
        "            for docid in docids:\n",
        "                doc = self.doc[docid]\n",
        "                final_result.append(\n",
        "                    {\n",
        "                        \"docid\": docid,\n",
        "                        \"title\": doc[\"title\"],\n",
        "                        \"url\": doc[\"url\"],\n",
        "                    }\n",
        "                )\n",
        "        else:\n",
        "            for score, docid in scores_docids:\n",
        "                doc = self.doc[docid]\n",
        "                final_result.append(\n",
        "                    {\n",
        "                        \"docid\": docid,\n",
        "                        \"title\": doc[\"title\"],\n",
        "                        \"url\": doc[\"url\"],\n",
        "                        \"score\": score,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        return final_result\n"
      ],
      "metadata": {
        "id": "PjvlEyez15id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Generic Scoring Function"
      ],
      "metadata": {
        "id": "Dod67smz6-PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the QueryProcessor with the input folder\n",
        "input_folder = \"/content/index/index.pkl\"\n",
        "query_processor = GenericQueryProcessor(input_folder)\n",
        "\n",
        "# Run the search engine UI\n",
        "search_engine_ui(query_processor=query_processor)"
      ],
      "metadata": {
        "id": "TDL8dtD-J53e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Inverted Index and TF-IDF Query Processor"
      ],
      "metadata": {
        "id": "0OT7z5SMwQVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Inverted Index"
      ],
      "metadata": {
        "id": "woCCVL5L7pXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TFIDFInvertedIndex(GenericInvertedIndex):\n",
        "\n",
        "    def __init__(self, lex, inv, doc, stats):\n",
        "        super().__init__(lex, inv, doc, stats)\n",
        "\n",
        "    def get_posting(self, termid):\n",
        "        # Find the term corresponding to the given termid\n",
        "        term = next(key for key, value in self.lexicon.items() if value[0] == termid)\n",
        "\n",
        "        # Get the doc_freq (document frequency) from the lexicon\n",
        "        doc_freq = self.lexicon[term][1]\n",
        "\n",
        "        # Get the total number of documents\n",
        "        total_docs = self.stats[\"num_docs\"]\n",
        "\n",
        "        return TFIDFInvertedIndex.PostingListIterator(\n",
        "            self.inv[\"docids\"][termid],\n",
        "            self.inv[\"freqs\"][termid],\n",
        "            self.doc,\n",
        "            doc_freq,\n",
        "            total_docs\n",
        "        )\n",
        "\n",
        "\n",
        "    class PostingListIterator(GenericInvertedIndex.PostingListIterator):\n",
        "\n",
        "        def __init__(self, docids, freqs, doc, doc_freq, total_docs):\n",
        "            super().__init__(docids, freqs, doc)\n",
        "            self.doc_freq = doc_freq\n",
        "            self.total_docs = total_docs\n",
        "\n",
        "        def score(self):\n",
        "            if self.is_end_list():\n",
        "                    return 0\n",
        "            docid = self.docids[self.pos]\n",
        "            tf = self.freqs[self.pos]  # Term frequency in the document\n",
        "            doclen = self.doc[docid][\"doclen\"]  # Document length\n",
        "            if self.doc_freq == 0:  # Avoid division by zero\n",
        "                return 0\n",
        "            idf = math.log(self.total_docs / self.doc_freq)\n",
        "            tf = 1 + (math.log(tf))\n",
        "            return tf * idf"
      ],
      "metadata": {
        "id": "rxNlyxiEvCc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Querying Processor"
      ],
      "metadata": {
        "id": "32I7bfzZ955L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TFIDFQueryProcessor(GenericQueryProcessor):\n",
        "\n",
        "    def __init__(self, index_file):\n",
        "        super().__init__(index_file)\n",
        "        self.inv_index = TFIDFInvertedIndex(self.lex, self.inv, self.doc, self.stats)"
      ],
      "metadata": {
        "id": "oW7qk-Xk9-ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing TF-IDF Scoring Function"
      ],
      "metadata": {
        "id": "oT-UW9fWAWdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the QueryProcessor with the input folder\n",
        "input_folder = \"/content/index/index.pkl\"\n",
        "query_processor = TFIDFQueryProcessor(input_folder)\n",
        "\n",
        "# Run the search engine UI\n",
        "search_engine_ui(query_processor=query_processor)"
      ],
      "metadata": {
        "id": "Wj835XV5A9w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BM25 Inverted Index and BM25 Query Processor"
      ],
      "metadata": {
        "id": "7ruM8iMzDHb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BM25 Inverted Index"
      ],
      "metadata": {
        "id": "WlnLZCcIx5Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BM25InvertedIndex(GenericInvertedIndex):\n",
        "\n",
        "    def __init__(self, lex, inv, doc, stats):\n",
        "        super().__init__(lex, inv, doc, stats)\n",
        "\n",
        "    def get_posting(self, termid):\n",
        "        # Find the term corresponding to the given termid\n",
        "        term = next(key for key, value in self.lexicon.items() if value[0] == termid)\n",
        "\n",
        "        # Get the doc_freq (document frequency) from the lexicon\n",
        "        doc_freq = self.lexicon[term][1]\n",
        "\n",
        "        # Get the total number of documents\n",
        "        total_docs = self.stats[\"num_docs\"]\n",
        "\n",
        "      # Calculate the Avg Document Length\n",
        "        avgdoclen = self.stats[\"total_tokens\"] / self.stats[\"num_docs\"]\n",
        "\n",
        "        return BM25InvertedIndex.PostingListIterator(\n",
        "            self.inv[\"docids\"][termid],\n",
        "            self.inv[\"freqs\"][termid],\n",
        "            self.doc,\n",
        "            doc_freq,\n",
        "            total_docs,\n",
        "            avgdoclen\n",
        "        )\n",
        "\n",
        "    class PostingListIterator(GenericInvertedIndex.PostingListIterator):\n",
        "\n",
        "        def __init__(self, docids, freqs, doc, doc_freq, total_docs,avgdoclen, k1 = 1.2, b = 0.75):\n",
        "            super().__init__(docids, freqs, doc)\n",
        "            self.doc_freq = doc_freq\n",
        "            self.total_docs = total_docs\n",
        "            self.avgdoclen = avgdoclen\n",
        "            self.k1 = k1\n",
        "            self.b = b\n",
        "\n",
        "\n",
        "        def score(self):\n",
        "            if self.is_end_list():\n",
        "                return 0\n",
        "            docid = self.docids[self.pos]\n",
        "            tf = self.freqs[self.pos]  # Term frequency in the document\n",
        "            doclen = self.doc[docid][\"doclen\"]  # Document length\n",
        "            idf = math.log((self.total_docs - self.doc_freq + 0.5) / (self.doc_freq + 0.5) + 1)\n",
        "\n",
        "            # BM25 score calculation\n",
        "            numerator = tf * (self.k1 + 1)\n",
        "            denominator = tf + (self.k1 * (1 - self.b + (self.b * (doclen / self.avgdoclen))))\n",
        "            return idf * (numerator / denominator)"
      ],
      "metadata": {
        "id": "tjYqDnVzx4kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BM25 Querying Processor"
      ],
      "metadata": {
        "id": "D0YosYy_FTTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BM25QueryProcessor(GenericQueryProcessor):\n",
        "\n",
        "    def __init__(self, index_file):\n",
        "        super().__init__(index_file)\n",
        "        self.inv_index = BM25InvertedIndex(self.lex, self.inv, self.doc, self.stats)"
      ],
      "metadata": {
        "id": "maWK5JkavCY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing BM25 Scoring Function"
      ],
      "metadata": {
        "id": "mAV0WhlIFyJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the QueryProcessor with the input folder\n",
        "input_folder = \"/content/index/index.pkl\"\n",
        "query_processor = BM25QueryProcessor(input_folder)\n",
        "\n",
        "# Run the search engine UI\n",
        "search_engine_ui(query_processor=query_processor)"
      ],
      "metadata": {
        "id": "j8qsXDM-FwyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRIBhj1zAQm7"
      },
      "source": [
        "# V- Evaluation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61ecf45e9535463f83a6e0556e1b4974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee76b053100743388577ac030f706de1",
              "IPY_MODEL_4128af8a1e5d44beb92ce241fc29b1f7",
              "IPY_MODEL_953974af1ea141b2b82248edc847b172"
            ],
            "layout": "IPY_MODEL_c914bbcefc3d43eeaf5562a05d980f4e"
          }
        },
        "ee76b053100743388577ac030f706de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5fc4f741fe04732ade872ca5f0df1f9",
            "placeholder": "​",
            "style": "IPY_MODEL_159573d1f0f54125a31c75fc2856a4a4",
            "value": "Indexing Files:   0%"
          }
        },
        "4128af8a1e5d44beb92ce241fc29b1f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_559dd8be89c7405ca1bd261cf38afbf3",
            "max": 34,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffeafdd2f8524b4195561c2a696dd35b",
            "value": 0
          }
        },
        "953974af1ea141b2b82248edc847b172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc0cfd13d46e4c48ba519839feb41098",
            "placeholder": "​",
            "style": "IPY_MODEL_401f1024141d47d3b161b95e240f78af",
            "value": " 0/34 [00:00&lt;?, ?it/s]"
          }
        },
        "c914bbcefc3d43eeaf5562a05d980f4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5fc4f741fe04732ade872ca5f0df1f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "159573d1f0f54125a31c75fc2856a4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "559dd8be89c7405ca1bd261cf38afbf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffeafdd2f8524b4195561c2a696dd35b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc0cfd13d46e4c48ba519839feb41098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "401f1024141d47d3b161b95e240f78af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}